{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KUL H02A5a Computer Vision: Group Assignment 2\n",
    "---------------------------------------------------------------\n",
    "Student numbers: <span style=\"color:red\">r0974957, r0975376, r0674083, r0687031, r0811736</span>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview <a id='overview'></a>\n",
    "Brief (interactive) overview of the notebook:\n",
    "\n",
    "* [1 Overview](#overview) \n",
    "    * [1.1 Environment setup & Deep learning resources](#deep-learning-resources) \n",
    "    * [1.2 Dataset setup](#dataset-setup) \n",
    "    * [1.3 Kaggle submission](#kaggle-submission)\n",
    "* [2 Image classification](#image-classification)\n",
    "    * [2.1 Image pre-processing](#image-pre-processing)\n",
    "    * [2.2 Model training/validation](#vgg16)\n",
    "* **TODO be finished once everything is completely completely done**\n",
    "\n",
    "Brief summary of the project:   \n",
    "\n",
    "In the first part, you train an end-to-end neural network for image classification. In the second part, you will do the same for semantic segmentation. For these two tasks we expect you to put a significant effort into optimizing performance and as such competing with fellow students via the Kaggle competition. In the third part, you will try to find and exploit the weaknesses of your classification and/or segmentation network. For the latter there is no competition format, but we do expect you to put significant effort in achieving good performance on the self-posed goal for that part. Finally, we ask you to reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Sect. 5). It is important to note that only a small part of the grade will reflect the actual performance of your networks. However, we do expect all things to work! In general, we will evaluate the correctness of your approach and your understanding of what you have done that you demonstrate in the descriptions and discussions in the final notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Environment setup & Deep learning resources <a id='deep-learning-resources'></a>\n",
    "For our project, we have made extensive use of both Keras, Tensorflow and PyTorch frameworks. In addition, we used some extra libraries to calculate losses and automate generating some of our other training/validation metrics. Because the models, compared to last project, were much more computatinally expensive, we have also experimented with GPU and cloud acceleration in Cuda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping into all of that, we prepare our Kaggle environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ext_dataset = \"content/dataset_upload/\"         # In Kaggle we can add a dataset to the kernel, so we can use this path to access the dataset\n",
    "path_to_train = \"data/train/train/\"                     # -> We can have our csv files there, as well as the images and potentially weights\n",
    "path_to_test = \"data/test/test/\"    # Set both train and test paths to the last \"test\" or \"train\" folder in the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Python3 Docker environment: https://github.com/kaggle/docker-python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import cv2 as cv\n",
    "\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataset setup <a id='dataset-setup'></a>\n",
    "For this project we used the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. The dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes.\n",
    "\n",
    "*The functions from this section were all provided in the notebook template.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training data \n",
    "#/content/drive/MyDrive/CV_DeepLearning/data/train/train/train_set.csv\n",
    "train_df = pd.read_csv(path_to_train + 'train_set.csv', index_col=\"Id\")\n",
    "labels = train_df.columns\n",
    "train_df[\"img\"] = [np.load(path_to_train + 'img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "train_df[\"seg\"] = [np.load(path_to_train + 'seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "print(\"The training set contains {} examples.\".format(len(train_df)))\n",
    "\n",
    "# Show some examples\n",
    "fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n",
    "for i, label in enumerate(labels):\n",
    "    df = train_df.loc[train_df[label] == 1]\n",
    "    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n",
    "    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n",
    "    axs[1, i].axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test data\n",
    "test_df = pd.read_csv(path_to_test + 'test_set.csv', index_col=\"Id\")\n",
    "test_df[\"img\"] = [np.load(path_to_test + 'img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n",
    "test_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\n",
    "print(\"The test set contains {} examples.\".format(len(test_df)))\n",
    "\n",
    "# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Kaggle submission <a id='kaggle-submission'></a>\n",
    "*Likewise, this code was provided in the template. It is called later on in our notebook to create the final submission file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rle_encode(img):\n",
    "    \"\"\"\n",
    "    Kaggle requires RLE encoded predictions for computation of the Dice score (https://www.kaggle.com/lifa08/run-length-encode-and-decode)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img: np.ndarray - binary img array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rle: String - running length encoded version of img\n",
    "    \"\"\"\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    rle = ' '.join(str(x) for x in runs)\n",
    "    return rle\n",
    "\n",
    "def generate_submission(df):\n",
    "    \"\"\"\n",
    "    Make sure to call this function once after you completed Sect. 2 and Sect. 3! It transforms and writes your test dataframe into a submission.csv file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame - filled dataframe that needs to be converted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    submission_df: pd.DataFrame - df in submission format.\n",
    "    \"\"\"\n",
    "    df_dict = {\"Id\": [], \"Predicted\": []}\n",
    "    for idx, _ in df.iterrows():\n",
    "        df_dict[\"Id\"].append(f\"{idx}_classification\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array(df.loc[idx, labels])))\n",
    "        df_dict[\"Id\"].append(f\"{idx}_segmentation\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array([df.loc[idx, \"seg\"] == j + 1 for j in range(len(labels))])))\n",
    "\n",
    "    submission_df = pd.DataFrame(data=df_dict, dtype=str).set_index(\"Id\")\n",
    "    submission_df.to_csv(\"submission.csv\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image classification <a id='image-classification'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = train_df[\"img\"].to_numpy()\n",
    "trainY = train_df[labels].to_numpy()\n",
    "print(f\"trainX shape: {trainX.shape}\")\n",
    "\n",
    "testX = test_df[\"img\"].to_numpy()\n",
    "print(f\"testX shape: {testX.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the provided images, we will first apply rescaling and interpolation. This allows us to use batches of same-sized tensors as inputs to our model during validation and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [cv.resize(image, (224,224), interpolation = cv.INTER_LINEAR) for image in trainX]                     # train images \n",
    "images_test = [cv.resize(image_test, (224,224), interpolation = cv.INTER_LINEAR) for image_test in testX]       # test images\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10 * 2, 10))\n",
    "axs[0].imshow(images[0], vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Example train image\", fontsize=20)\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(images_test[0], vmin=0, vmax=255)\n",
    "axs[1].set_title(\"Example test image\", fontsize=20)\n",
    "axs[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images already look good! Their aspect ratio is slightly distorted, but the reshape is necessary for some of the models we have investigated. We can keep the stretching/squeezing in mind when making conclusions later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# convert the image pixels to a numpy array\n",
    "images = [img_to_array(image) for image in images]\n",
    "images_test = [img_to_array(image_test) for image_test in images_test]\n",
    "\n",
    "images = [image.reshape((image.shape[0], image.shape[1], image.shape[2])) for image in images]\n",
    "images_test = [image_test.reshape((image_test.shape[0], image_test.shape[1], image_test.shape[2])) for image_test in images_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models we will investigate for this section were trained on the ImageNet dataset, which contained circa 10000 classes. \n",
    "\n",
    "The first of such model is **VGG16**. On initial inspection we noticed that we **needed additional pre-processing** to make our model fit the data. For instance, without this additional step, if presented with a image of a horse, the model classified it as a great dane (black dog breed). \n",
    "\n",
    "To enhance our dataset, we apply zero-centering to our data with the preprocess_input function used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# prepare the image for the VGG model\n",
    "images = np.array([preprocess_input(image) for image in images])\n",
    "images_test = np.array([preprocess_input(image_test) for image_test in images_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to further investigate the distribution of classes in our dataset. Is it imbalanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = images.copy()\n",
    "trainY = train_df[labels].to_numpy()\n",
    "\n",
    "testX = images_test.copy()\n",
    "\n",
    "print(f\"trainX shape: {trainX.shape}, trainY shape: {trainY.shape}\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(np.arange(1,21,1),np.sum(trainY,axis=0))\n",
    "plt.xlabel(\"Class number\")\n",
    "plt.ylabel(\"Instance counts\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is immediately clear that class 15 (\"person\") is present much more than the other classes. In addition, the others have less than 100 samples, which isn't ideal even when using transfer learning. To counter the imbalance and lack of instances, we will augment the other classes and increase their size. After augmentation we will split the data into the training and validation set with a ratio of 9-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General data augmentation\n",
    "\n",
    "tf.random.set_seed(42)  # For reproducibility\n",
    "\n",
    "data_aug = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "  tf.keras.layers.RandomContrast(.2),\n",
    "  tf.keras.layers.RandomBrightness(.2),\n",
    "  tf.keras.layers.RandomZoom(.2)\n",
    "])\n",
    "\n",
    "finalX=trainX.copy()\n",
    "finalY=trainY.copy()\n",
    "indices=np.where(trainY[:,14]==0)[0]  # don't select the 15th (over-represented) class \n",
    "\n",
    "filteredX=trainX[indices]\n",
    "filteredY=trainY[indices]\n",
    "\n",
    "n_runs=5\n",
    "for i in range(n_runs):\n",
    "    finalX=np.concatenate((finalX,data_aug(filteredX)))\n",
    "    finalY=np.concatenate((finalY,filteredY))\n",
    "\n",
    "trainX,valX, trainY,valY = train_test_split(finalX, finalY, test_size=.1, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(np.arange(1,21,1),np.sum(trainY,axis=0),label=\"Training data\")\n",
    "plt.bar(np.arange(1,21,1),np.sum(valY,axis=0),bottom=np.sum(trainY,axis=0),label=\"Validation data\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Class number\")\n",
    "plt.ylabel(\"Instance counts\")\n",
    "plt.xticks(np.arange(1,21,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model training/validation <a id='vgg16'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training/validation/testing data all set up, we can start our training. \n",
    "\n",
    "We will start with the aforementioned **VGG16 model**. On top of that, we create a dense neural network after its convolutional layers.\n",
    "\n",
    "The model was compiled with Adam optimizer and $\\eta=0.001$. Sigmoid activations were placed at the outputs to allow for independant probability estimates. By default, VGG has softmax activations, and is hence used for single-label classifications. This is not the case here, as one image can have multiple labels. However, those images are in a minority (585 out of 3113 training images). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "tf.random.set_seed(42)      # For reproducibility\n",
    "\n",
    "base_model = VGG16()\n",
    "for layer in base_model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add dense layers at the end of the base model\n",
    "x = Dense(1024, activation='relu')(base_model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "predictions = Dense(20, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.001), \n",
    "    loss='categorical_focal_crossentropy', metrics=[tf.keras.metrics.F1Score()])\n",
    "model.summary()     # Get overview of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has circa 6.7 milion parameters, just for future model reference. Batchsize was chosen to be 128, as lower values of 64, e.g., produce very noisy gradients. Larger batch values are computationaly more heavy and do not contribute to loss improvement. \n",
    "\n",
    "The optimized loss function is the so called focal loss, or categorical focal crossentropy. It is related to categorical crossentropy, but it is more adapted to imbalanced datasets, like ours. When we tried to optimize the standard crossentropy, it got stuck relatively quickly. \n",
    "\n",
    "As a performance indicator, we use the F1-score. It is a harmonic mean of precision and recall. It provides a more balanced assessment of a classifier dealing with imbalanced data. A perfect classifier has the F1-score of 1. \n",
    "\n",
    "Without further adue, we set the training to commence! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit \n",
    "# Reference time: 10min with L4 TPU, 62.8GB RAM\n",
    "\n",
    "# # Training \n",
    "# history=model.fit(trainX, trainY.astype(\"float32\"), batch_size=128, epochs=15,\n",
    "#           validation_data=(valX,valY.astype(\"float32\")))\n",
    "\n",
    "# model.save_weights('/content/drive/MyDrive/cv/group_assignment/content/dataset_upload/vgg16_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vgg16_history.png](images/vgg16_history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always save/load our model weights to avoid re-running the training often. For this notebook, we will simply load the weights from a previous run. \n",
    "\n",
    "Several edits to the added dense layers' layout while observing runs, in addition to tuning general parameters such as epoch, batch size, etc. \n",
    "In the end, there were two models that stuck out for us, one Resnet implementation, the other VGG16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# TODO question: How to I best load these models? \n",
    "\n",
    "# Final models: \n",
    "model_vgg = keras.saving.load_model(path_to_ext_dataset + 'vgg_model1.keras')\n",
    "model_res = keras.saving.load_model(path_to_ext_dataset + 'resnet_best.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we predict the outputs. We do that is by thresholding the sigmoid activations with the value $\\eta=0.99$. However, not all images have probabilites above this value. If that is the case, we take the largest probability and assign the image to that class alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat=model_res.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat1 = yhat.copy()\n",
    "thresh=.99\n",
    "yhat1=yhat1>thresh\n",
    "yhat1=yhat1.astype(\"uint8\")\n",
    "\n",
    "for i in range(len(yhat1)):\n",
    "    if np.sum(yhat1[i])==0:\n",
    "        yhat1[i] = (yhat[i]>=max(yhat[i])).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the predicted values we can generate the first part of our submission file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = test_df.columns.values[:-2]\n",
    "test_df[cols] = yhat1\n",
    "test_df.seg = [np.zeros(s.shape) for s in test_df.seg]\n",
    "generate_submission(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: How much of the following things should I keep? how should I best describe these? \n",
    "\n",
    "# thresh = 0.99\n",
    "# index=3\n",
    "# #np.sum((yhat>thresh).astype(\"uint8\") & (valY==1),axis=(0,1))/np.sum(valY==1,axis=(0,1))\n",
    "# print(yhat[index])\n",
    "# print(yhat[index]>thresh)\n",
    "# print(trainY[index])\n",
    "\n",
    "# from keras.applications.vgg16 import decode_predictions\n",
    "# # convert the probabilities to class labels\n",
    "# label = decode_predictions(yhat)\n",
    "# # retrieve the most likely result, e.g. highest probability\n",
    "# label = label[0][0]\n",
    "# # print the classification\n",
    "# print('%s (%.2f%%)' % (label[1], label[2]*100))\n",
    "\n",
    "# from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# w=compute_sample_weight(class_weight='balanced', y=trainY)\n",
    "# w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "# custom_model=tf.keras.Sequential([\n",
    "#     tf.keras.layers.Rescaling(1./255,input_shape=(224,224,3)),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(4,(5,5),activation=\"relu\"),\n",
    "#     tf.keras.layers.Conv2D(4,(5,5),activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(16,(5,5),activation=\"relu\"),\n",
    "#     tf.keras.layers.Conv2D(16,(5,5),activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(64,(5,5),activation=\"relu\"),\n",
    "#     tf.keras.layers.Conv2D(64,(5,5),activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(256,(5,5),activation=\"relu\"),\n",
    "#     tf.keras.layers.Conv2D(256,(5,5),activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "#     tf.keras.layers.Conv2D(256,(5,5),activation=\"relu\"),\n",
    "\n",
    "\n",
    "\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     # tf.keras.layers.Dense(256,activation=\"relu\"),\n",
    "#     # #tf.keras.layers.Dense(1024,activation=\"relu\"),\n",
    "#     # #tf.keras.layers.Dense(1024,activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(2048,activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(1024,activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(256,activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(20,activation=\"sigmoid\")\n",
    "# ])\n",
    "\n",
    "# custom_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.00001), \n",
    "#     loss='categorical_focal_crossentropy', metrics=[tf.keras.metrics.F1Score()])\n",
    "# custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(41)\n",
    "\n",
    "# data_aug1 = tf.keras.Sequential([\n",
    "#   tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "#   tf.keras.layers.RandomRotation(0.2),\n",
    "#   tf.keras.layers.RandomContrast(.2),\n",
    "#   tf.keras.layers.RandomBrightness(.2),\n",
    "# ])\n",
    "\n",
    "# finalX=trainX.copy()\n",
    "# finalY=trainY.copy()\n",
    "# filteredX=trainX.copy()\n",
    "# filteredY=trainY.copy()\n",
    "# n_runs=2\n",
    "# for i in range(n_runs):\n",
    "#     finalX=np.concatenate((finalX,data_aug(filteredX)))\n",
    "#     finalY=np.concatenate((finalY,filteredY))\n",
    "\n",
    "# trainX,valX,trainY,valY=train_test_split(finalX,finalY,test_size=.1,random_state=42)\n",
    "# plt.bar(np.arange(1,21,20),np.sum(trainY,axis=0),label=\"Training data\")\n",
    "# plt.bar(np.arange(1,21,20),np.sum(valY,axis=0),bottom=np.sum(trainY,axis=0),label=\"Validation data\")\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"Class number\")\n",
    "# plt.ylabel(\"Instance counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO These are extremely fast to run again, so I can run them again if needed. \n",
    "\n",
    "# history1=custom_model.fit(trainX, trainY.astype(\"float32\"), batch_size=128, epochs=150,\n",
    "#           validation_data=(valX,valY.astype(\"float32\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.semilogy(history1.history[\"val_loss\"])\n",
    "# plt.semilogy(history1.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Semantic segmentation\n",
    "Our goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (and/or background). We use the training set to train your CNN and compete on the test set (by filling in the segmentation column in the test dataframe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above text hints, we are no now looking at each individual pixel's class. This also means that we now consider object area for our class distribution.\n",
    "\n",
    "We can, for example, imagine that background might take up the largest number of pixels in our image. We can plot the pixel-per-class histogram to see if that is indeed the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = np.zeros(len(labels) + 1)      # We consider both labels + a background class \n",
    "\n",
    "# We iterate the provided training segmentation masks \n",
    "# -> How many pixels belong to each class?\n",
    "for _, row in train_df.iterrows():\n",
    "    for i, label in enumerate(labels):\n",
    "        class_distribution[i+1] += np.sum(row[\"seg\"] == i+1)\n",
    "    class_distribution[0] += np.sum(row[\"seg\"] == 0)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(range(len(labels) + 1), class_distribution)\n",
    "plt.ylabel(\"Number of pixels\")\n",
    "plt.xlabel(\"Class index\")                                                   # Class 0 = background, remaining are in order of the labels \n",
    "plt.title(\"Training class-pixel distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the background takes up the overwhelming majority of pixels. This makes the dataset very inbalanced, something we have to take into account during our training later on. With an inproper loss, the model would be content simply guessing 'background' for each pixel. \n",
    "\n",
    "We can create a second histogram without the background class, to inspect the distribution inbetween labels better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(range(1, len(labels) + 1), class_distribution[1:])\n",
    "plt.ylabel(\"Number of pixels\")\n",
    "plt.xlabel(\"Class index\")\n",
    "plt.title(\"Training class-pixel distribution (no background)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only looking at PASCAL VOC labels, the training dataset is already much more balanced. We still have one class that sticks out, at index 15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The label that still sticks out is: --{labels[14]}--\")   # Label at index 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The person class being abundantly present is also to our expectation. During the image classification phase of this project, we also noticed that many images included people, sometimes even multiple on a single image. \n",
    "\n",
    "To take into account the imbalance of classes later on, we already create an inverse class distribution list. We will use this list later on to scale the importance of each class to calculate our loss. \n",
    "- Class occurs more often -> smaller importance on loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "total_pixels = np.sum(class_distribution)\n",
    "inverse_class_distribution = total_pixels / class_distribution\n",
    "inverse_class_distribution = torch.Tensor(inverse_class_distribution)\n",
    "print(\"inverse class distribution:\\n\")\n",
    "print(f\"background: \\t{inverse_class_distribution[0]}\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{label}:    \\t{inverse_class_distribution[i+1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our training and validation set, as well as their respective dataloaders. \n",
    "\n",
    "We need two different transforms for the image and the segmentation. Both need to be converted to tensors, but we will also normalize the images. This normalization is done based on very common values for the Imagenet dataset. We decided these normalization values would also apply to our dataset, as Imagenet is representative of a large number of common images. \n",
    "\n",
    "TODO change description \n",
    "Something very different from our image classification section is the use of a custom **collate** function. We DO NOT want to reshape the images, to make the output segmentation map the same shape (other than colour channel dimension) as our input image. Thi scustom collate function doesn't stack the images into one tensor (which would not be possible), but instead adds them all to a list. \n",
    "\n",
    "We select the batch size of 4 images. Most of the training was done on our personal devices using GPUs (Cuda implementation), so we had to take into account the memory limits this brings with it. Additionally, small micro-batches are often a good choice for segmentation model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have to reverse the reshape operation and want to lose as little info as possible. Therefore we will select the best resize shape based on the shapes of the training images.\n",
    "shapes = np.array([img.shape for img in train_df[\"seg\"]])\n",
    "# Get the average shape\n",
    "avg_shape = np.mean(shapes, axis=0)\n",
    "print(f\"The average shape of the training segmentation masks is: {avg_shape}\")\n",
    "# We will resize to 416x416, a common size for segmentation tasks and one fitting the average shape \n",
    "# It is essential to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the images and segmentation maps to torch tensors\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform_img, transform_seg):\n",
    "        self.df = df\n",
    "        self.transform_img = transform_img      # transform for the images\n",
    "        self.transform_seg = transform_seg      # transform for the segmentation maps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.transform_img(self.df.iloc[idx][\"img\"])\n",
    "        seg = self.transform_seg(self.df.iloc[idx][\"seg\"])\n",
    "\n",
    "        # one-hot encoding for the segmentation map makes it easier for us later on \n",
    "        seg = one_hot(seg.to(torch.int64), num_classes=len(labels) + 1).squeeze(0).permute(2, 0, 1).float()\n",
    "\n",
    "        return img, seg\n",
    "    \n",
    "transform_img = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                                                # Convert to PIL image\n",
    "    transforms.Resize((416, 416), interpolation=transforms.InterpolationMode.BILINEAR),     # Bilinear interpolation for high quality \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),            # Imagenet normalization values \n",
    "])\n",
    "\n",
    "transform_seg = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                                                # Convert to PIL image\n",
    "    transforms.Resize((416, 416), interpolation=transforms.InterpolationMode.NEAREST),      # Only nearest neighbour (segmentation map should not be interpolated)\n",
    "    # transforms.ToTensor()                 # DO NOT USE ToTensor(), as it will convert the segmentation map to [0, 1] range \n",
    "    transforms.Lambda(lambda x: torch.Tensor(np.array(x)))                          \n",
    "])\n",
    "\n",
    "# Custom collate \n",
    "# # # # # # # # # # \n",
    "def collate_fn(batch):                      # Can be used if we don't want to rescale our images, then tensors can't be stacked \n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    return [data, target]                   # Returned as a datatype we know [ data, target ] \n",
    "# # # # # # # # # #                         # -> if used, should be taken into account in the model (no single batch)\n",
    "\n",
    "train_dataset = CustomDataset(train_df, transform_img, transform_seg)\n",
    "val_dataset = CustomDataset(val_df, transform_img, transform_seg)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)#, collate_fn=collate_fn)        # Selected batch size of 16\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)#, collate_fn=collate_fn)           # We don't shuffle the validation set  \n",
    "\n",
    "print(f\"The training dataset contains {len(train_dataset)} examples\")\n",
    "print(f\"The validation dataset contains {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our model, we decide on the scores to use. \n",
    "For our segmentation section, we will will use: \n",
    "- Dice score \n",
    "- Mean IoU \n",
    "\n",
    "These are two commonly used scores in image segmentation. They both express a measure of overlap between the predicted and training segmentation map. In addition, dice score is used for the Kaggle competition for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # Workaround problem importing segmentation_models \n",
    "%env SM_FRAMEWORK=tf.keras      \n",
    "# # # # # # \n",
    "\n",
    "from tensorflow import keras\n",
    "import segmentation_models as sm\n",
    "                                                # Two main scores:\n",
    "IoU_score = sm.metrics.IOUScore(threshold=0.5)  # - IoU                         Both functions provide default smooth of 1e-05, to avoid division by zero \n",
    "dice_score = sm.metrics.FScore(threshold=0.5)   # - Dice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics above are used during training in the keras framework. To manually use them on individual images, we need to define them ourselves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_IoU_score(y_true, y_pred, smooth=1e-5, threshold=0.5):\n",
    "    \n",
    "    # y_pred_binary = np.where(y_pred > threshold, 1, 0)      # Binarize the predictions using threshold\n",
    "\n",
    "    # Binarize the predictions using argmax \n",
    "    y_pred_binary = np.zeros_like(y_pred)\n",
    "    y_pred_binary[np.arange(y_pred.shape[0])[:, None], np.argmax(y_pred, axis=1).flatten()] = 1\n",
    "\n",
    "    \n",
    "    iou_per_class = []\n",
    "\n",
    "    for i in range(y_true.shape[1]):                                                            # Iterate over class channels\n",
    "\n",
    "        y_true_flatten, y_pred_flatten = y_true[:, i].flatten(), y_pred_binary[:, i].flatten()        # Flatten for easier calculations \n",
    "\n",
    "        # Simple IoU formula \n",
    "        intersection = np.sum(y_true_flatten * y_pred_flatten)\n",
    "        union = np.sum(y_true_flatten) + np.sum(y_pred_flatten) - intersection\n",
    "\n",
    "        iou = (intersection + smooth) / (union + smooth)    # I smoothed the values to avoid division by zero \n",
    "        iou_per_class.append(iou)\n",
    "\n",
    "    return np.mean(iou_per_class)\n",
    "\n",
    "def custom_dice_score(y_true, y_pred, smooth=1e-5, threshold=0.8):\n",
    "\n",
    "    y_pred_binary = np.where(y_pred > threshold, 1, 0)      \n",
    "\n",
    "    # y_pred_binary = np.zeros_like(y_pred)\n",
    "    # y_pred_binary[np.arange(y_pred.shape[0]), np.argmax(y_pred, axis=1)] = 1\n",
    "\n",
    "    dice_per_class = []\n",
    "\n",
    "    for i in range(y_true.shape[0]):\n",
    "\n",
    "        y_true_flatten, y_pred_flatten = y_true[:, i].flatten(), y_pred_binary[:, i].flatten()        \n",
    "\n",
    "        # Similar approach to the IoU score, slightly different formula \n",
    "        intersection = np.sum(y_true_flatten * y_pred_flatten)\n",
    "        dice = (2 * intersection + smooth) / (np.sum(y_true_flatten) + np.sum(y_pred_flatten) + smooth)\n",
    "\n",
    "        dice_per_class.append(dice)\n",
    "\n",
    "    return np.mean(dice_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data y_true and y_pred\n",
    "y_true = np.ones((16, 21, 416, 416))\n",
    "y_pred_rand = np.random.rand(16, 21, 416, 416)\n",
    "y_pred_one = np.ones((16, 21, 416, 416))\n",
    "\n",
    "\n",
    "# Calculate the custom IoU and Dice scores\n",
    "print(f\"Custom IoU score for random predictions: {custom_IoU_score(y_true, y_pred_rand)}\")\n",
    "print(f\"Custom Dice score for random predictions: {custom_dice_score(y_true, y_pred_rand)}\")\n",
    "print(f\"Custom IoU score for ones predictions: {custom_IoU_score(y_true, y_pred_one)}\")\n",
    "print(f\"Custom Dice score for ones predictions: {custom_dice_score(y_true, y_pred_one)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have a way to check if our model is taking the inbalance in the distribution into account. It would be easy to simply guess \"background\" for each pixel and get a semi-decent result, but this is not what we want from a good segmentation mask. Hence, we check what scores a \"background only\" model would get, to compare to our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BackgroundModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BackgroundModel, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tremp_background_tensor = torch.zeros(x.shape[0], 21, x.shape[2], x.shape[3])           # Create tensor of zeros with shape (batch, 1, height, width)\n",
    "        tremp_background_tensor[:, 0] = 1                                                       # Set the class channel (background) to 1 (will be the highest value)\n",
    "        return tremp_background_tensor\n",
    "\n",
    "background_model = BackgroundModel()        \n",
    "background_model(torch.zeros(16, 3, 416, 416)).shape    # Quick test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulate the IoU and Dice score for the background model\n",
    "mean_IoU = 0\n",
    "mean_dice = 0\n",
    "\n",
    "for (data_batch, target_batch) in val_loader:\n",
    "    output = background_model(data_batch)\n",
    "\n",
    "    mean_IoU += custom_IoU_score(target_batch.numpy(), output.detach().numpy())\n",
    "    mean_dice += custom_dice_score(target_batch.numpy(), output.detach().numpy())\n",
    "\n",
    "mean_IoU /= len(val_loader)\n",
    "mean_dice /= len(val_loader)\n",
    "\n",
    "print(f\"The mean IoU score for the background model is {mean_IoU}\")\n",
    "print(f\"The mean Dice score for the background model is {mean_dice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, just guessing background provides a relatively good result. For our final model, we wish to beat this by a significant margin. \n",
    "\n",
    "Now that we have defined our scores and a baseline value for them, we can define define the loss we will use to train our models. For this, we opted for focal loss, a variant of cross-entropy loss, which better takes into account the inbalance of our dataset in addition to brining a greater focus to badly-classified images.\n",
    "\n",
    "We defined the function for this ourselves. This provides us the most flexibility. Additionally, this ensures that our loss functions within the Pytorch frameweork. Pytorch computes backpropagenation based on graph traversal, and defining the loss ourselves ensures that the loss is never decoupled from it's graph history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None):       # default gamma == 2 recommended by focal loss paper authors \n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = target.long().argmax(dim=1)        \n",
    "        cross_loss = F.cross_entropy(input, target)      # we start with standard cross-entropy loss \n",
    "\n",
    "        pt = torch.exp(-cross_loss)\n",
    "        focal_loss = -(1 - pt) ** self.gamma * torch.log(pt + 1e-5)         # Added small 1e-5 bias to avoid log(0) \n",
    "        # As we can see for the formula, pt < 0.5 will increase the loss, pt > 0.5 will decrease the loss. Gamma will increase the difference between the two \n",
    "        # This produces the desired effect of focusing on hard examples \n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_factor = self.alpha[target]\n",
    "            focal_loss = focal_loss * alpha_factor\n",
    "\n",
    "        return focal_loss.mean()     # Chose to do a mean instead of sum reduction, this prevents the loss from being too high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focalloss = FocalLoss()\n",
    "y_mask = torch.zeros(16, 21, 416, 416)\n",
    "y_mask[:, 1] = 1\n",
    "\n",
    "y_pred = torch.zeros(16, 21, 416, 416)\n",
    "y_pred[:, 1] = 10       # Something larger than 1 gives better results \n",
    "\n",
    "loss = focalloss(y_pred, y_mask)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pass the inverse class distribution we created earlier as the alpha for our focal loss. This will allow us to take into account the inbalance during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_loss = FocalLoss(gamma=2, alpha=inverse_class_distribution)\n",
    "focal_loss_no_alpha = FocalLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly demonstrate the influence of the alpha on the loss calculation. As we expect, the \"only background\" model has a much greater loss once we take into account the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterer_train = iter(train_loader)\n",
    "temp_data, temp_seg = next(iterer_train)\n",
    "\n",
    "temp_output = background_model(temp_data)\n",
    "\n",
    "temp_loss_no_alpha = focal_loss_no_alpha(temp_output, temp_seg)\n",
    "temp_loss = focal_loss(temp_output, temp_seg)\n",
    "print(f\"The focal loss without alpha is {temp_loss_no_alpha}\")\n",
    "print(f\"The focal loss with alpha is {temp_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to define our model. Because of our success using resnet for the image classification section of the project, we first explored two models that use this modes as a backbone. \n",
    "\n",
    "The model in question: \n",
    "- FCN-ResNet \n",
    "\n",
    "In our first exploration, we initialized only the backbone weights (ResNet trained on ImageNetV1). This means that all other weights are randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import fcn_resnet101\n",
    "\n",
    "from torchvision.models import ResNet101_Weights\n",
    "\n",
    "res_weights = ResNet101_Weights.IMAGENET1K_V1\n",
    "model_fcn = fcn_resnet101(weights=None, progress=False, num_classes=len(labels)+1, weights_backbone=res_weights)\n",
    "\n",
    "print(model_fcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we edited the model output dimension to fit our 21 classes. After discovering the num_classes parameter in our model initialisation, we no longer need to worry about doing this manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fcn.classifier[4] = nn.Conv2d(512, len(labels)+1, kernel_size=(1, 1), stride=(1, 1))      # manual last layer edit to output size \n",
    "# nn.init.xavier_uniform_(model_fcn.classifier[4].weight)                                         # manual last layer parameter initialization \n",
    "# print(model_fcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cuda on our local machine allows for faster processing times. This is essential with large models such as the ones we are working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device {device}\")\n",
    "\n",
    "# torch.cuda.empty_cache()    # Empty the cache to avoid memory errors\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "device = \"cpu\"      # default, no difficulties with memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_class_distribution = inverse_class_distribution.to(device)\n",
    "focal_loss = FocalLoss(gamma=2, alpha=inverse_class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training loop takes into account all the design decisions we have made thusfar. Each image is processed individually because of the custom collate function and we get our loss from a custom loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, light_mode: bool = False):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (img_batch, seg_batch) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_batch = 0.0\n",
    "\n",
    "        img_batch, seg_batch = img_batch.to(device), seg_batch.to(device)   # Move data to cuda or cpu \n",
    "        \n",
    "        if (light_mode):\n",
    "            with torch.cuda.amp.autocast():                     # Mixed precision auto-cast for model forward pass and loss calculation\n",
    "                output = model(img_batch)[\"out\"]                     \n",
    "                loss_batch = criterion(output, seg_batch)     \n",
    "\n",
    "            scaler.scale(loss_batch).backward()                 # Mixed precision backpropagation\n",
    "            scaler.step(optimizer)                              # Mixed precision optimizer step\n",
    "            scaler.update()\n",
    "\n",
    "            del loss_batch, img_batch, seg_batch, output\n",
    "            torch.cuda.empty_cache()   \n",
    "\n",
    "        else:\n",
    "            output = model(img_batch)[\"out\"]                     \n",
    "            loss_batch = criterion(output, seg_batch)\n",
    "\n",
    "            loss_batch.backward()                               # Standard backpropagation\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss_batch.item()\n",
    "        print(f\"\\rBatch {i}/{len(train_loader)}, Loss: {loss_batch}\", end=\"\")\n",
    "\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0.0\n",
    "    mean_IoU_val = 0.0\n",
    "    mean_dice_val = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_batch, seg_batch in val_loader:\n",
    "                \n",
    "            img_batch, seg_batch = img_batch.to(device), seg_batch.to(device)\n",
    "            output = model(img_batch)[\"out\"].detach()\n",
    "\n",
    "            loss_val += criterion(output, seg_batch)\n",
    "\n",
    "            output = output.cpu().numpy()\n",
    "            seg_batch = seg_batch.cpu().numpy()\n",
    "            \n",
    "            mean_IoU_val += custom_IoU_score(seg_batch, output)\n",
    "            mean_dice_val += custom_dice_score(seg_batch, output)\n",
    "\n",
    "    loss_val /= len(val_dataset)\n",
    "    mean_IoU_val /= len(val_dataset)\n",
    "    mean_dice_val /= len(val_dataset)\n",
    "\n",
    "    return loss_val, mean_IoU_val, mean_dice_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a very common apprach to our optimizer, opting for Adam with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_fcn.parameters(), lr=0.001)\n",
    "model_fcn = model_fcn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it is time for us to train our models! \n",
    "\n",
    "We will make sure to record all necessary metrics in a file while training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new file in fcn_models/ to save the epoch loss\n",
    "file = open(\"fcn_models/training_data.csv\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "\n",
    "nr_epochs = 2\n",
    "for epoch in range(nr_epochs):\n",
    "\n",
    "    # # # # # # # Training\n",
    "    model_fcn.train()\n",
    "    loss_train = train(model_fcn, train_loader, focal_loss, optimizer, device)\n",
    "\n",
    "    # # # # # # # Validation\n",
    "    model_fcn.eval()\n",
    "    loss_val, mean_IoU_val, mean_dice_val = validate(model_fcn, val_loader, focal_loss, device)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{nr_epochs}, Loss train: {loss_train}, Loss val: {loss_val}, IoU val: {mean_IoU_val}, Dice val: {mean_dice_val}\")\n",
    "    file.write(f\"{epoch},{loss_train},{loss_val},{mean_IoU_val},{mean_dice_val}\\n\")\n",
    "\n",
    "    # save the temp model every 4 epochs\n",
    "    if epoch % 4 == 0:\n",
    "        torch.save(model_fcn.state_dict(), f\"fcn_models/model_epoch_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the file \n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with the best validation loss\n",
    "model_fcn.load_state_dict(torch.load(\"fcn_models/model_epoch_0.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "\n",
    "from torchvision.models import ResNet101_Weights\n",
    "\n",
    "res_weights = ResNet101_Weights.IMAGENET1K_V1\n",
    "model_deep = deeplabv3_resnet101(weights=None, progress=False, num_classes=len(labels)+1, weights_backbone=res_weights)\n",
    "\n",
    "print(model_fcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD THE REST OF THIS SECTION HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_seg_data(image_size=128):\n",
    "    \"\"\"Loads and preprocesses segmentation data.\n",
    "\n",
    "    Args:\n",
    "      image_size: The desired size for resizing images and segmentations.\n",
    "\n",
    "    Returns:\n",
    "      A tuple containing:\n",
    "          images: A list of preprocessed images (normalized to 0-1).\n",
    "          segmentations: A list of one-hot encoded segmentation masks.\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_csv(path_train + 'train_set.csv', index_col=\"Id\")\n",
    "    labels = train_df.columns\n",
    "\n",
    "    # Load images and segmentations using pandas vectorized access\n",
    "    train_df[\"img\"] = [np.load(path_train + 'img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "    train_df[\"seg\"] = [np.load(path_train + 'seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "\n",
    "    images = []\n",
    "    segmentations = []\n",
    "\n",
    "    for row in range(len(train_df)):\n",
    "        images.append(cv2.resize(train_df.at[row, \"img\"], (image_size,image_size))/ 255.)\n",
    "        segmentations.append(cv2.resize(train_df.at[row, \"seg\"], (image_size,image_size)))\n",
    "        \n",
    "    # One-hot encode segmentations\n",
    "    segmentations = tf.keras.utils.to_categorical(segmentations, num_classes=21)\n",
    "\n",
    "    return images, segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_df[\"seg\"]\n",
    "distribution = np.mean([[np.sum(Y_ == i) / Y_.size for i in range(len(labels) + 1)] for Y_ in Y], axis=0)\n",
    "print(\"Distribution:\")\n",
    "print(f\"background:{distribution[0]*100}%\")\n",
    "for i in range(len(distribution)-1):\n",
    "    print(f\"{labels[i]}: {distribution[i+1]*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_segnet_model(input_shape=(128, 128, 3)):\n",
    "    \"\"\"Builds a SegNet model for image segmentation.\n",
    "\n",
    "    Args:\n",
    "      input_shape: The shape of the input image (height, width, channels).\n",
    "\n",
    "    Returns:\n",
    "      A compiled SegNet model.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder block (with downsampling)\n",
    "    def encoder_block(x, filters):\n",
    "        x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "        return x\n",
    "\n",
    "    # Decoder block (with upsampling)\n",
    "    def decoder_block(x, filters):\n",
    "        x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.UpSampling2D((2, 2))(x)\n",
    "        return x\n",
    "\n",
    "    # Encoder\n",
    "    encoded = encoder_block(inputs, 64)\n",
    "    encoded = encoder_block(encoded, 128)\n",
    "    encoded = encoder_block(encoded, 256)\n",
    "    encoded = encoder_block(encoded, 512)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = decoder_block(encoded, 512)\n",
    "    decoded = decoder_block(decoded, 256)\n",
    "    decoded = decoder_block(decoded, 128)\n",
    "    decoded = decoder_block(decoded, 64)\n",
    "\n",
    "    # Output layer with softmax activation for segmentation mask\n",
    "    outputs = layers.Conv2D(21, (3, 3), padding='same', activation=\"softmax\")(decoded)\n",
    "\n",
    "    # Create the model\n",
    "    segnet_model = tf.keras.Model(inputs, outputs, name=\"SegNet\")\n",
    "\n",
    "    return segnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(6 * 20, 15 * 2))\n",
    "for i, label in enumerate(labels[6:11]):\n",
    "    df = train_df.loc[train_df[label] == 1]\n",
    "    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n",
    "    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n",
    "    axs[1, i].axis(\"off\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models as sm\n",
    "IoU_score = sm.metrics.IOUScore(threshold=0.5)\n",
    "dice_score = sm.metrics.FScore(threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X_seg, train_y_seg = load_and_preprocess_seg_data()\n",
    "train_seg_X, val_seg_X, train_seg_y, val_seg_y = train_test_split(train_X_seg, train_y_seg, test_size=0.05)\n",
    "\n",
    "\n",
    "if False:\n",
    "    segnet_model = build_segnet_model(input_shape=(128,128,3))\n",
    "\n",
    "    # Compile the model\n",
    "    segnet_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                      loss=sm.losses.categorical_focal_dice_loss,  \n",
    "                      metrics=['accuracy', dice_score, IoU_score,])  \n",
    "\n",
    "    model_history = segnet_model.fit(np.array(train_seg_X), np.array(train_seg_y), epochs=30, batch_size = 5,\n",
    "                                     validation_data=(np.array(val_seg_X), np.array(val_seg_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(3 * 3, 3 * 3))\n",
    "    for index in range(3):\n",
    "        axs[0, index].imshow(val_seg_X[index], vmin=0, vmax=255)\n",
    "        axs[1, index].imshow(np.argmax(val_seg_y[index],-1), vmin=0, vmax=20) \n",
    "        axs[2, index].imshow(np.argmax(segnet_model.predict(np.expand_dims(val_seg_X[index], axis=0))[0],-1), vmin=0, vmax=20)\n",
    "        axs[0, index].axis(\"off\")\n",
    "        axs[1, index].axis(\"off\")\n",
    "        axs[2, index].axis(\"off\")\n",
    "        axs[0, index].set_title(\"Image\")\n",
    "        axs[1, index].set_title(\"Segment Mask\")\n",
    "        axs[2, index].set_title(\"Prediction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "    x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "    f = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "    p = layers.MaxPool2D(2)(f)\n",
    "    return f, p\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "    # Upsample\n",
    "    x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
    "    \n",
    "    # Concatenate\n",
    "    x = layers.concatenate([x, conv_features])\n",
    "    \n",
    "    # Conv2D twice with ReLU activation\n",
    "    x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "    x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "    return x\n",
    "\n",
    "def build_unet_model(input_shape=(128,128,3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    # Downsample\n",
    "    f1, p1 = downsample_block(inputs, 32)\n",
    "    f2, p2 = downsample_block(p1, 64)\n",
    "    f3, p3 = downsample_block(p2, 128)\n",
    "    f4, p4 = downsample_block(p3, 256)\n",
    "    \n",
    "    # Bottleneck: Conv2D twice with ReLU activation\n",
    "    bottleneck = layers.Conv2D(512, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(p4)\n",
    "    bottleneck = layers.Conv2D(512, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(bottleneck)\n",
    "    \n",
    "    # Decoder\n",
    "    # Upsample\n",
    "    u6 = upsample_block(bottleneck, f4, 256)\n",
    "    u7 = upsample_block(u6, f3, 128)\n",
    "    u8 = upsample_block(u7, f2, 64)\n",
    "    u9 = upsample_block(u8, f1, 32)\n",
    "    \n",
    "    outputs = layers.Conv2D(21, 1, padding=\"same\", activation = \"softmax\")(u9)\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs, name=\"U-Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    train_X_seg, train_y_seg = load_and_preprocess_seg_data()\n",
    "    train_seg_X, val_seg_X, train_seg_y, val_seg_y = train_test_split(train_X_seg, train_y_seg, test_size=0.05)\n",
    "\n",
    "    unet_model = build_unet_model()\n",
    "\n",
    "    unet_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                      loss=sm.losses.categorical_focal_dice_loss,  \n",
    "                      metrics=['accuracy', dice_score, IoU_score,])  \n",
    "\n",
    "    model_history = unet_model.fit(np.array(train_seg_X), np.array(train_seg_y), epochs=1, batch_size = 5,\n",
    "                                     validation_data=(np.array(val_seg_X), np.array(val_seg_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(3 * 3, 3 * 3))\n",
    "    for index in range(3):\n",
    "        axs[0, index].imshow(val_seg_X[index], vmin=0, vmax=255)\n",
    "        axs[1, index].imshow(np.argmax(val_seg_y[index],-1), vmin=0, vmax=20) \n",
    "        axs[2, index].imshow(np.argmax(unet_model.predict(np.expand_dims(val_seg_X[index], axis=0))[0],-1), vmin=0, vmax=20)\n",
    "        axs[0, index].axis(\"off\")\n",
    "        axs[1, index].axis(\"off\")\n",
    "        axs[2, index].axis(\"off\")\n",
    "        axs[0, index].set_title(\"Image\")\n",
    "        axs[1, index].set_title(\"Segment Mask\")\n",
    "        axs[2, index].set_title(\"Prediction\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNet with VGG16 encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "def build_unet_vgg_model(input_shape=(128, 128, 3), num_classes=21):\n",
    "    # Load VGG16 model pretrained on ImageNet without fully connected layers\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    # Block 1\n",
    "    f1, p1 = downsample_block(vgg16.input, 64)\n",
    "    # Block 2\n",
    "    f2, p2 = downsample_block(p1, 128)\n",
    "    # Block 3\n",
    "    f3, p3 = downsample_block(p2, 256)\n",
    "    # Block 4\n",
    "    f4, p4 = downsample_block(p3, 512)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = layers.Conv2D(1024, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(p4)\n",
    "    bottleneck = layers.Conv2D(1024, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(bottleneck)\n",
    "    \n",
    "    # Decoder\n",
    "    # Upsample\n",
    "    u6 = upsample_block(bottleneck, f4, 512)\n",
    "    u7 = upsample_block(u6, f3, 256)\n",
    "    u8 = upsample_block(u7, f2, 128)\n",
    "    u9 = upsample_block(u8, f1, 64)\n",
    "    \n",
    "    outputs = layers.Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(u9)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=vgg16.input, outputs=outputs, name=\"VGG16_U-Net\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    train_X_seg, train_y_seg = load_and_preprocess_seg_data()\n",
    "    train_seg_X, val_seg_X, train_seg_y, val_seg_y = train_test_split(train_X_seg, train_y_seg, test_size=0.05)\n",
    "\n",
    "    unet_vgg_model = build_unet_vgg_model()\n",
    "\n",
    "    unet_vgg_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                      loss=sm.losses.categorical_focal_dice_loss,  \n",
    "                      metrics=['accuracy', dice_score, IoU_score,])  \n",
    "\n",
    "    model_history = unet_vgg_model.fit(np.array(train_seg_X), np.array(train_seg_y), epochs=100, batch_size = 5,\n",
    "                                     validation_data=(np.array(val_seg_X), np.array(val_seg_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_vgg_model.save_weights('unet_vgg_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNet with EfficientNet encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet.tfkeras import EfficientNetB0  # Import EfficientNetB0 from the efficientnet package\n",
    "\n",
    "def build_unet_efficientnet_model(input_shape=(128, 128, 3), num_classes=21):\n",
    "    # Load EfficientNetB0 model pretrained on ImageNet\n",
    "    efficientnet = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    \n",
    "    # Encoder\n",
    "    # The block outputs are stored for skip connections\n",
    "    block1, p1 = downsample_block(efficientnet.input, 32)\n",
    "    block2, p2 = downsample_block(p1, 64)\n",
    "    block3, p3 = downsample_block(p2, 128)\n",
    "    block4, p4 = downsample_block(p3, 256)\n",
    "    block5, p5 = downsample_block(p4, 512)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = layers.Conv2D(1024, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(p5)\n",
    "    bottleneck = layers.Conv2D(1024, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(bottleneck)\n",
    "    \n",
    "    # Decoder\n",
    "    # Upsample\n",
    "    u6 = upsample_block(bottleneck, block5, 512)\n",
    "    u7 = upsample_block(u6, block4, 256)\n",
    "    u8 = upsample_block(u7, block3, 128)\n",
    "    u9 = upsample_block(u8, block2, 64)\n",
    "    u10 = upsample_block(u9, block1, 32)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(u10)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=efficientnet.input, outputs=outputs, name=\"EfficientNet_U-Net\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    train_X_seg, train_y_seg = load_and_preprocess_seg_data()\n",
    "    train_seg_X, val_seg_X, train_seg_y, val_seg_y = train_test_split(train_X_seg, train_y_seg, test_size=0.05)\n",
    "\n",
    "    unet_efficientnet_model = build_unet_efficientnet_model()\n",
    "\n",
    "    unet_efficientnet_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                      loss=sm.losses.categorical_focal_dice_loss,  \n",
    "                      metrics=['accuracy', dice_score, IoU_score,])  \n",
    "\n",
    "    model_history = unet_efficientnet_model.fit(np.array(train_seg_X), np.array(train_seg_y), epochs=100, batch_size = 5,\n",
    "                                     validation_data=(np.array(val_seg_X), np.array(val_seg_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_efficientnet_model.save_weights('unet_efficientnet_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNet with ResNet50 encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_resnet_model(input_shape=(128, 128, 3), num_classes=21):\n",
    "    # Load ResNet model pretrained on ImageNet\n",
    "    resnet = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        classes=num_classes,\n",
    "        classifier_activation='softmax',\n",
    "    )\n",
    "    \n",
    "    # Encoder\n",
    "    # The block outputs are stored for skip connections\n",
    "    block1, p1 = downsample_block(resnet.input, 32)\n",
    "    block2, p2 = downsample_block(p1, 64)\n",
    "    block3, p3 = downsample_block(p2, 128)\n",
    "    block4, p4 = downsample_block(p3, 256)\n",
    "    block5, p5 = downsample_block(p4, 512)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = layers.Conv2D(1024, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(p5)\n",
    "    bottleneck = layers.Conv2D(1024, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(bottleneck)\n",
    "    \n",
    "    # Decoder\n",
    "    # Upsample\n",
    "    u6 = upsample_block(bottleneck, block5, 512)\n",
    "    u7 = upsample_block(u6, block4, 256)\n",
    "    u8 = upsample_block(u7, block3, 128)\n",
    "    u9 = upsample_block(u8, block2, 64)\n",
    "    u10 = upsample_block(u9, block1, 32)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(u10)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=resnet.input, outputs=outputs, name=\"ResNet_U-Net\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    train_X_seg, train_y_seg = load_and_preprocess_seg_data()\n",
    "    train_seg_X, val_seg_X, train_seg_y, val_seg_y = train_test_split(train_X_seg, train_y_seg, test_size=0.05)\n",
    "\n",
    "    unet_resnet_model = build_unet_resnet_model()\n",
    "\n",
    "    unet_resnet_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                      loss=sm.losses.categorical_focal_dice_loss,  \n",
    "                      metrics=['accuracy', dice_score, IoU_score,])  \n",
    "\n",
    "    model_history = unet_resnet_model.fit(np.array(train_seg_X), np.array(train_seg_y), epochs=100, batch_size = 5,\n",
    "                                     validation_data=(np.array(val_seg_X), np.array(val_seg_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_resnet_model.save_weights('unet_resnet_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet prediction plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(3 * 3, 3 * 3))\n",
    "    for index in range(3):\n",
    "        axs[0, index].imshow(val_seg_X[index], vmin=0, vmax=255)\n",
    "        axs[1, index].imshow(np.argmax(val_seg_y[index],-1), vmin=0, vmax=20) \n",
    "        axs[2, index].imshow(np.argmax(unet_efficientnet_model.predict(np.expand_dims(val_seg_X[index], axis=0))[0],-1), vmin=0, vmax=20)\n",
    "        axs[0, index].axis(\"off\")\n",
    "        axs[1, index].axis(\"off\")\n",
    "        axs[2, index].axis(\"off\")\n",
    "        axs[0, index].set_title(\"Image\")\n",
    "        axs[1, index].set_title(\"Segment Mask\")\n",
    "        axs[2, index].set_title(\"Prediction\")\n",
    "    \n",
    "    plt.savefig('efficientNet_predictions.jpg')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to competition\n",
    "You don't need to edit this section. Just use it at the right position in the notebook. See the definition of this function in Sect. 1.3 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adversarial attack\n",
    "For this part, your goal is to fool your classification and/or segmentation CNN, using an *adversarial attack*. More specifically, the goal is build a CNN to perturb test images in a way that (i) they look unperturbed to humans; but (ii) the CNN classifies/segments these images in line with the perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 ResNet model training/validation <a id='resnet'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We temporarily train on only bus and car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"/content/train/\"\n",
    "\n",
    "train_df = pd.read_csv(path_train+'train_set.csv', index_col=\"Id\")\n",
    "train_df[\"img\"] = [np.load(path_train+'img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "train_df[\"seg\"] = [np.load(path_train+'seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "\n",
    "class1 = 'bus'\n",
    "class2 = 'car'\n",
    "\n",
    "filtered_train_df = train_df[((train_df[class1] == 1) & (train_df[class2] == 0)) | ((train_df[class1] == 0) & (train_df[class2] == 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "\n",
    "for i in filtered_train_df.index:\n",
    "    filtered_train_df.at[i, 'img'] = cv.resize(filtered_train_df.at[i, 'img'],(image_size,image_size))/255.0\n",
    "\n",
    "images_train = [row.img for row in filtered_train_df.itertuples()]\n",
    "labels_train =[0 if getattr(row, class1, None) == 1 else 1 for row in filtered_train_df.itertuples()]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(images_train, labels_train, test_size=0.10)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_X))\n",
    "print(\"Number of validation examples:\", len(val_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import InceptionResNetV2\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "resNet = InceptionResNetV2(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(image_size,image_size, 3),\n",
    "    pooling='avg',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the weights of all layers\n",
    "resNet.trainable = False\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    resNet,\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "train_y_inverted = np.where(np.array(train_y) == 0, 1, 0)\n",
    "val_y_inverted = np.where(np.array(val_y) == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "# Reference time: 20s with A100 TPU, 83.5GB RAM\n",
    "\n",
    "# # training\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "# model.fit(np.array(train_X), train_y_inverted, batch_size=16, epochs=20, validation_data=(np.array(val_X), val_y_inverted), verbose=1)\n",
    "# model.save_weights(\"/content/drive/MyDrive/cv/group_assignment/content/dataset_upload/resnet_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"/content/drive/MyDrive/cv/group_assignment/content/dataset_upload/resnet_weights.h5\")\n",
    "\n",
    "[loss, accuracy] = model.evaluate(np.array(val_X), np.array(val_y_inverted), batch_size=16, verbose=0)\n",
    "\n",
    "print(f\"Val Loss: {loss}\")\n",
    "print(f\"Val Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D,BatchNormalization,Activation,MaxPooling2D,UpSampling2D,Input\n",
    "\n",
    "def encoder(i,inp):\n",
    "    res = Conv2D(i, (3, 3), padding='same')(inp)\n",
    "    res = BatchNormalization()(res)\n",
    "    res = Activation('relu')(res)\n",
    "    return MaxPooling2D((2, 2), padding='same')(res)\n",
    "\n",
    "def decoder(i,inp):\n",
    "    res = Conv2D(i, (3, 3), padding='same')(inp)\n",
    "    res = BatchNormalization()(res)\n",
    "    res = Activation('relu')(res)\n",
    "    return UpSampling2D((2, 2))(res)\n",
    "\n",
    "image_size = 128\n",
    "input_shape = Input(shape=(image_size,image_size,3))\n",
    "\n",
    "x = encoder(64,input_shape)\n",
    "x = encoder(32,x)\n",
    "x = encoder(16,x)\n",
    "\n",
    "x = decoder(16,x)\n",
    "x = decoder(32,x)\n",
    "x = decoder(64,x)\n",
    "\n",
    "# convert back to rgb image\n",
    "\n",
    "x = Conv2D(3, (3, 3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "adv_net = Activation('sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.math import multiply,reduce_max,abs,divide\n",
    "\n",
    "def rescale_max_norm(x):\n",
    "    return multiply(0.2, divide(x, reduce_max(abs(x))))\n",
    "\n",
    "def rescale_max_norm_small(x):\n",
    "    return multiply(0.07, divide(x, reduce_max(abs(x))))\n",
    "\n",
    "def rescale_max_norm_large(x):\n",
    "    return multiply(0.5, divide(x, reduce_max(abs(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "import tensorflow.keras.backend as bck\n",
    "\n",
    "perturb_max = Lambda(rescale_max_norm, output_shape=(image_size,image_size,3), name='perturb_max')(adv_net)\n",
    "\n",
    "def add_perturbation_to_img(x):\n",
    "    perturbation,input_img = x\n",
    "    return bck.clip(perturbation+input_img,0,1)\n",
    "\n",
    "add_perturbation_max = Lambda(add_perturbation_to_img, output_shape=(image_size,image_size,3), name='add_perturb_max')([perturb_max,input_shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False\n",
    "\n",
    "max_norm_output = model(add_perturbation_max)\n",
    "\n",
    "checkpoint_filepath_max = '/kaggle/working/max.weights.h5'\n",
    "max_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_max,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "checkpoint_filepath_max_small = '/kaggle/working/max_small.weights.h5'\n",
    "max_small_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_max_small,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "checkpoint_filepath_max_large = '/kaggle/working/max_large.weights.h5'\n",
    "max_large_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_max_large,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_model_max = Model(inputs=input_shape, outputs=max_norm_output)\n",
    "\n",
    "adversary_model_max.compile(optimizer=Adam(learning_rate=0.001),loss=BinaryCrossentropy(from_logits=False), metrics = ['accuracy'])\n",
    "\n",
    "max_history = adversary_model_max.fit(np.array(train_X), np.array(train_y), batch_size=16, epochs=250,\n",
    "                        validation_data=(np.array(val_X), \n",
    "                        np.array(val_y)),\n",
    "                        verbose=1,\n",
    "                        callbacks=[max_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_model_max.load_weights(checkpoint_filepath_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[loss_model, accuracy_model] = model.evaluate(np.array(val_X), np.array(val_y_inverted), batch_size=16, verbose=0)\n",
    "\n",
    "print(f\"Validation loss model: {loss_model}\")\n",
    "print(f\"Validation accuracy: {accuracy_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[loss_max, acc_max] = adversary_model_max.evaluate(np.array(val_X), np.array(val_y), verbose=0)\n",
    "\n",
    "print(\"Validation loss max norm:\", loss_max)\n",
    "print(\"Validation accuracy max norm:\", acc_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adversary(adversary_model, interm_image, interm_perturbation,version):\n",
    "    \n",
    "    predictions_train = np.round(adversary_model.predict(np.array(train_X), verbose=0)).reshape(len(train_X))\n",
    "    predictions_val = np.round(adversary_model.predict(np.array(val_X), verbose=0)).reshape(len(val_X))\n",
    "\n",
    "    fooled_images_train = np.array(train_X)[abs(predictions_train - train_y_inverted) == 1]\n",
    "    fooled_labels_train = np.array(train_y)[abs(predictions_train - train_y_inverted) == 1]\n",
    "    if len(fooled_images_train)>0:\n",
    "        perturb_train = interm_perturbation.predict(fooled_images_train, verbose=0)\n",
    "\n",
    "    fooled_images_val = np.array(val_X)[abs(predictions_val - val_y_inverted) == 1]\n",
    "    fooled_labels_val = np.array(val_y)[abs(predictions_val - val_y_inverted) == 1]\n",
    "    if len(fooled_images_val)>0:\n",
    "        perturb_val = interm_perturbation.predict(fooled_images_val, verbose=0)\n",
    "    \n",
    "    if len(fooled_labels_train):\n",
    "        fig, axs = plt.subplots(3, 1 , figsize=(4 * 2, 4 * 2))\n",
    "        fig.suptitle('Training images'+version, fontsize=16)\n",
    "        \n",
    "        axs[0].imshow(interm_image.predict(np.expand_dims(fooled_images_train[0], axis=0), verbose=0)[0])\n",
    "        if fooled_labels_train[0] == 1:\n",
    "            axs[0].set_title(\"Model predicted: \"+class1+\", Actual: \"+class2 , fontsize=10)\n",
    "        else:\n",
    "            axs[0].set_title(\"Model predicted: \"+class2+\", Actual: \"+class1 , fontsize=10)\n",
    "        axs[0].axis(\"off\")\n",
    "        axs[1].imshow(fooled_images_train[0])\n",
    "        axs[1].axis(\"off\")\n",
    "        axs[2].imshow(perturb_train[0])\n",
    "        axs[2].axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    if len(fooled_images_val):\n",
    "        fig, axs = plt.subplots(3, 1 , figsize=(4 * 2, 4 * 2))\n",
    "        fig.suptitle('Validation images'+version, fontsize=16)\n",
    "        \n",
    "        axs[0].imshow(interm_image.predict(np.expand_dims(fooled_images_val[0], axis=0), verbose=0)[0])\n",
    "        if fooled_labels_val[0] == 1:\n",
    "            axs[0].set_title(\"Model predicted: \"+class1+\", Actual: \"+class2 , fontsize=10)\n",
    "        else:\n",
    "            axs[0].set_title(\"Model predicted: \"+class2+\", Actual: \"+class1 , fontsize=10)\n",
    "        axs[0].axis(\"off\")\n",
    "        axs[1].imshow(fooled_images_val[0])\n",
    "        axs[1].axis(\"off\")\n",
    "        axs[2].imshow(perturb_val[0])\n",
    "        axs[2].axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_pert_img_layer = Model(inputs=adversary_model_max.input,outputs=adversary_model_max.get_layer('add_perturb_max').output)\n",
    "intermediate_perturbation_layer = Model(inputs=adversary_model_max.input,outputs=adversary_model_max.get_layer('perturb_max').output)\n",
    "\n",
    "plot_adversary(adversary_model_max, intermediate_pert_img_layer, intermediate_perturbation_layer,\" Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_max_small = Lambda(rescale_max_norm_small, output_shape=(image_size,image_size,3), name='perturb_max_small')(adv_net)\n",
    "perturb_max_large = Lambda(rescale_max_norm_large, output_shape=(image_size,image_size,3), name='perturb_max_large')(adv_net)\n",
    "\n",
    "add_perturbation_max_small = Lambda(add_perturbation_to_img, output_shape=(image_size,image_size,3), name='add_perturb_max_small')([perturb_max_small,input_shape])\n",
    "add_perturbation_max_large = Lambda(add_perturbation_to_img, output_shape=(image_size,image_size,3), name='add_perturb_max_large')([perturb_max_large,input_shape])\n",
    "\n",
    "\n",
    "model.trainable = False\n",
    " \n",
    "max_norm_output_small = model(add_perturbation_max_small)\n",
    "max_norm_output_large = model(add_perturbation_max_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_model_max_small = Model(inputs=input_shape, outputs=max_norm_output_small)\n",
    "adversary_model_max_small.compile(optimizer=Adam(learning_rate=0.001),loss=BinaryCrossentropy(from_logits=False), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_small_history = adversary_model_max_small.fit(np.array(train_X), np.array(train_y), batch_size=16, epochs=500,\n",
    "                        validation_data=(np.array(val_X), \n",
    "                        np.array(val_y)),\n",
    "                        verbose=1,\n",
    "                        callbacks=[max_small_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_model_max_small.load_weights(max_small_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[loss_max, acc_max] = adversary_model_max_small.evaluate(np.array(val_X), np.array(val_y), verbose=0)\n",
    "\n",
    "print(\"Validation loss max norm:\", loss_max)\n",
    "print(\"Validation accuracy max norm:\", acc_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_pert_img_layer = Model(inputs=adversary_model_max_small.input,outputs=adversary_model_max_small.get_layer('add_perturb_max_small').output)\n",
    "intermediate_perturbation_layer = Model(inputs=adversary_model_max_small.input,outputs=adversary_model_max_small.get_layer('perturb_max_small').output)\n",
    "\n",
    "plot_adversary(adversary_model_max_small, intermediate_pert_img_layer, intermediate_perturbation_layer,\"Max small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_model_max_large = Model(inputs=input_shape, outputs=max_norm_output_large)\n",
    "adversary_model_max_large.compile(optimizer=Adam(learning_rate=0.001),loss=BinaryCrossentropy(from_logits=False), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_large_history = adversary_model_max_large.fit(np.array(train_X), np.array(train_y), batch_size=16, epochs=500,\n",
    "                        validation_data=(np.array(val_X), \n",
    "                        np.array(val_y)),\n",
    "                        verbose=1\n",
    "                        callbacks=[max_large_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_model_max_large.load_weights(max_large_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[loss_max, acc_max] = adversary_model_max_large.evaluate(np.array(val_X), np.array(val_y), verbose=0)\n",
    "\n",
    "print(\"Validation loss max norm:\", loss_max)\n",
    "print(\"Validation accuracy max norm:\", acc_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_pert_img_layer = tf.keras.Model(inputs=adversary_model_max_large.input,outputs=adversary_model_max_large.get_layer('add_perturb_max_large').output)\n",
    "intermediate_perturbation_layer = tf.keras.Model(inputs=adversary_model_max_large.input,outputs=adversary_model_max_large.get_layer('perturb_max_large').output)\n",
    "\n",
    "plot_adversary(adversary_model_max_large, intermediate_pert_img_layer, intermediate_perturbation_layer, \"Max Large\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV_group_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
